{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b0bf0c",
   "metadata": {},
   "source": [
    "## Artificial Neural Network to estimate pCO2sea ##\n",
    "## Produced by Carvalho, G. T. and Mejia, C., in 2024 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f708aa17-9c94-4c36-93fe-02a8ec046fee",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d636d-2412-469a-8161-01100ef535ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "from glob import glob \n",
    "import pickle\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from datetime import datetime  as dt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4693bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_n_model (input_dim, output_dim, hidden_dim, kernel_initializer='he_normal', activation='tanh', verbose=False):\n",
    "\n",
    "    if np.isscalar(hidden_dim):\n",
    "        hidden_dim = [hidden_dim]\n",
    "    \n",
    "    # create model\n",
    "    if verbose :\n",
    "        print('-- Initialize a keras Sequential model')\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Adding input layer\n",
    "    if verbose :\n",
    "        print(f\"-- adds input layer: {input_dim} cells\")\n",
    "    model.add(InputLayer(input_shape=(input_dim,)))\n",
    "\n",
    "    # Adding hidden layers\n",
    "    for ih,hid_dim in enumerate(hidden_dim):\n",
    "        if ih == 0 :\n",
    "            if verbose :\n",
    "                print(f\"-- adds the hidden layer {ih}: {hid_dim} cells, connected to input layer\")\n",
    "        else:\n",
    "            if verbose :\n",
    "                print(f\"-- adds the hidden layer: {hid_dim} cells, connected to previous one\")\n",
    "        model.add(Dense(units=hid_dim, kernel_initializer=kernel_initializer, activation=activation))\n",
    "\n",
    "    # Adding output layer\n",
    "    if verbose :\n",
    "        print(f\"-- adds the output layer: {output_dim} cells, connected to last hidden layer\")\n",
    "    model.add(Dense(units=output_dim, kernel_initializer=kernel_initializer))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df4120-a64c-46de-87c7-4fedbaeadc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajet_file = 'Import database.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff1be1-35fe-4758-8d37-4ca67cc08a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads current measured CSV data into a DataFrame\n",
    "print(f\"Reading boat data from file '{trajet_file}' ...\")\n",
    "socatt_df = pd.read_csv(trajet_file,sep=',',header='infer')\n",
    "\n",
    "socatt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a947883a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After reading CSV file, 'datetime' is readed as type STR. Here we convert column 'datetime' from STR to DATETIME type to deal it as a Time variable\n",
    "socatt_df['time'] = [dt.strptime(tt,'%d-%m-%Y') for tt in socatt_df['time'].values]\n",
    "\n",
    "# adds a YEAR column from the 'datetime' one\n",
    "socatt_df['YEAR'] = socatt_df['time'].dt.year\n",
    "socatt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a59559-1fcf-4cb1-a9f3-77bee7bcfae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retira los NaN\n",
    "socatt_df.dropna(inplace=True)\n",
    "socatt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d269dca8-982f-4113-861b-89c4c91a9a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "socatt_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7202d-6edf-4aee-ac67-e6e3891ec9ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "socatt_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197fc9ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plot data that will be used\n",
    "socatt_df.hist(bins=40,figsize=(18,8),layout=(4,8));\n",
    "plt.subplots_adjust(hspace=0.6)\n",
    "plt.suptitle(f\"{socatt_df.shape[0]} datos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28645fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "socatt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23aa352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fin de filtrages hay que recalcular el index del DataFrame\n",
    "socatt_df.reset_index(drop=True,inplace=True) # conserve l'ancien index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151348e9-e58d-49bb-95c1-6ef9eabecc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_vars_para_nn = ['sss', 'tsm', 'pco2atm','pco2ag']\n",
    "socatt_df[lista_vars_para_nn].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01f8741-cad0-4c36-a757-6fc45dc9147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  raw data variable names  |   normalized variable names\n",
    "# -------------------------   ----------------------------\n",
    "sst_varlabel = 'tsm';        sst_norm_varlabel  = 'sst_n'\n",
    "sss_varlabel = 'sss';        sss_norm_varlabel  = 'sal_n'\n",
    "pco2_varlabel = 'pco2ag';    pco2_norm_varlabel = 'pco2_n'\n",
    "pco2at_varlabel = 'pco2atm'; pco2at_norm_varlabel = 'pCO2_atm_n'\n",
    "# other variables\n",
    "time_varlabel = 'datetime'\n",
    "lat_varlabel = 'lat'\n",
    "lon_varlabel = 'lon'\n",
    "year_varlabel = 'YEAR'\n",
    "dayofyear_varlabel = 'dayOfYear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924bd361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization of Salinity (Sal or SSS) -> Centre-reduction\n",
    "# Computes MEAN and STD of Salinity\n",
    "sss_stat = [socatt_df[sss_varlabel].mean(), socatt_df[sss_varlabel].std()]\n",
    "# Apply it in order to normalize the values \n",
    "socatt_df[sss_norm_varlabel] = (socatt_df[sss_varlabel] - sss_stat[0])/sss_stat[1]\n",
    "\n",
    "# normalization of Sea Surface Temperature (SST) -> Centre-reduction\n",
    "sst_stat = [socatt_df[sst_varlabel].mean(), socatt_df[sst_varlabel].std()]\n",
    "socatt_df[sst_norm_varlabel] = (socatt_df[sst_varlabel] - sst_stat[0])/sst_stat[1]\n",
    "\n",
    "# normalization of pCO2 sea -> Centre-reduction\n",
    "pco2_stat = [socatt_df[pco2_varlabel].mean(), socatt_df[pco2_varlabel].std()]\n",
    "socatt_df[pco2_norm_varlabel] = (socatt_df[pco2_varlabel] - pco2_stat[0])/pco2_stat[1]\n",
    "\n",
    "# normalization of pCO2 air -> Centre-reduction\n",
    "pco2at_stat = [socatt_df[pco2at_varlabel].mean(), socatt_df[pco2at_varlabel].std()]\n",
    "socatt_df[pco2at_norm_varlabel] = (socatt_df[pco2at_varlabel] - pco2at_stat[0])/pco2at_stat[1]\n",
    "\n",
    "# We save in a dictionary the characteristics of the normalization for each variable\n",
    "case_dic['sss_stat'] = sss_stat\n",
    "case_dic['sst_stat'] = sst_stat\n",
    "case_dic['pco2_stat'] = pco2_stat\n",
    "case_dic['pco2at_stat'] = pco2_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768a7ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using directly the `pairplot()` function from `seaborn` module.\n",
    "[time_varlabel, lat_varlabel, lon_varlabel, sst_varlabel, sss_varlabel, pco2_varlabel,pco2at_varlabel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc6453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import initializers, activations\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer,Dense\n",
    "#from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint #, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ab61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of normalized variables composing Input and output\n",
    "vars_for_model_input = [sss_norm_varlabel,sst_norm_varlabel,pco2at_norm_varlabel] \n",
    "vars_for_model_output = [pco2_norm_varlabel]\n",
    "\n",
    "case_dic['input_vars'] = vars_for_model_input\n",
    "case_dic['output_vars'] = vars_for_model_output\n",
    "\n",
    "print(f\"\\nInput variables :  {vars_for_model_input}\")\n",
    "print(f\"Output variables : {vars_for_model_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7559a126-707d-4227-abe0-c9c1d14c251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "socatt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff89685e-0f84-4160-8686-1599e8963e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New DataFrame having case Input and output variables plus Time variable only\n",
    "data_test_separated = True\n",
    "\n",
    "if data_test_separated :\n",
    "    # Datas de Inicio y de Fin del TEST set\n",
    "    \n",
    "    test_ident = \"Choose file name\"; test_time_min = pd.to_datetime('choose a start date, example: 2008-01-01').strftime('%Y-%m-%d'); test_time_max = pd.to_datetime('choose an end date, example: 2010-12-31').strftime('%Y-%m-%d')  # Ano 2010\n",
    "    #test_ident = \"Jun2014'-Feb2015\"; test_time_min = pd.to_datetime('2014-06-01').strftime('%Y-%m-%d'); test_time_max = pd.to_datetime('2015-2-28').strftime('%Y-%m-%d')   # junio 2014 a fev 2015\n",
    "    \n",
    "    test_df = socatt_df.loc[lambda df: df['time'] >= test_time_min].loc[lambda df: df['time'] <= test_time_max ]   #NON# .reset_index()\n",
    "    train_df = pd.concat([socatt_df.loc[lambda df: df['time'] < test_time_min], socatt_df.loc[lambda df: df['time'] > test_time_max]],\n",
    "                          ignore_index=False)\n",
    "else:\n",
    "    train_df = socattOK_df[[time_varlabel]+vars_for_model_input+vars_for_model_output]\n",
    "\n",
    "# compute a random permuted index ou shuffle the lines ofdata\n",
    "n_patt = train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8891be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "iperm_randon = np.random.permutation(n_patt)\n",
    "\n",
    "# create a new DataFrame with random reordered lines of source TRAIN DataFrame \n",
    "train_rnd_df = train_df.iloc[iperm_randon,:].copy()\n",
    "\n",
    "train_rnd_df.reset_index(drop=False,inplace=True) # conserve l'ancien index\n",
    "train_rnd_df = train_rnd_df.rename(columns={\"index\": \"raw_index\"})\n",
    "#display(train_rnd_df)\n",
    "\n",
    "if data_test_separated :\n",
    "    test_seq_df = test_df.copy()\n",
    "    test_seq_df.reset_index(drop=False,inplace=True) # conserve l'ancien index\n",
    "    test_seq_df = test_seq_df.rename(columns={\"index\": \"raw_index\"})\n",
    "    #display(test_seq_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dce808-0038-489a-84c7-ff57dc8a7921",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_test_separated :\n",
    "    val_split = 0.15\n",
    "\n",
    "    train_label = 'TRAIN'\n",
    "    val_label   = 'VAL'\n",
    "    test_label  = 'TEST'\n",
    "\n",
    "    n_val = int(val_split * n_patt)\n",
    "    n_train = n_patt - n_val\n",
    "\n",
    "    train_rnd_df['dataset'] = [train_label]*n_train + [val_label]*n_val\n",
    "\n",
    "    n_test = test_df.shape[0]\n",
    "    \n",
    "    test_seq_df['dataset'] = [test_label]*n_test\n",
    "\n",
    "    train_rnd_df = pd.concat((train_rnd_df,test_seq_df), axis=0,\n",
    "                             ignore_index=True)\n",
    "else:\n",
    "    val_split = 0.15\n",
    "    test_split = 0.15\n",
    "\n",
    "    train_label = 'TRAIN'\n",
    "    val_label   = 'VAL'\n",
    "    test_label   = 'TEST'\n",
    "\n",
    "    n_val = int(val_split * n_patt)\n",
    "    n_test = int(test_split * n_patt)\n",
    "    n_train = n_patt - n_val - n_test\n",
    "\n",
    "    train_rnd_df['dataset'] = [train_label]*n_train + [val_label]*n_val + [test_label]*n_test\n",
    "\n",
    "train_rnd_df = train_rnd_df.rename(columns={\"index\": \"raw_index\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_rnd_df.loc[lambda df: df['dataset'] == train_label,vars_for_model_input].values\n",
    "Y_train = train_rnd_df.loc[lambda df: df['dataset'] == train_label,vars_for_model_output].values\n",
    "X_train_raw_index = train_rnd_df.loc[lambda df: df['dataset'] == train_label].index.values\n",
    "\n",
    "X_val = train_rnd_df.loc[lambda df: df['dataset'] == val_label,vars_for_model_input].values\n",
    "Y_val = train_rnd_df.loc[lambda df: df['dataset'] == val_label,vars_for_model_output].values\n",
    "X_val_raw_index = train_rnd_df.loc[lambda df: df['dataset'] == val_label].index.values\n",
    "\n",
    "if data_test_separated :\n",
    "    X_test = train_rnd_df.loc[lambda df: df['dataset'] == test_label,vars_for_model_input].values\n",
    "    Y_test = train_rnd_df.loc[lambda df: df['dataset'] == test_label,vars_for_model_output].values\n",
    "    X_test_raw_index = train_rnd_df.loc[lambda df: df['dataset'] == test_label].index.values\n",
    "\n",
    "else:\n",
    "    X_test = train_rnd_df.loc[lambda df: df['dataset'] == test_label,vars_for_model_input].values\n",
    "    Y_test = train_rnd_df.loc[lambda df: df['dataset'] == test_label,vars_for_model_output].values\n",
    "    X_test_raw_index = train_rnd_df.loc[lambda df: df['dataset'] == test_label,'raw_index'].values\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\\nY_train.shape: {Y_train.shape}\")\n",
    "print(f\"X_val shape:   {X_val.shape}\\nY_val.shape:   {Y_val.shape}\")\n",
    "print(f\"X_test shape:   {X_test.shape}\\nY_test.shape:   {Y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f55a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw_index.max(),train_rnd_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa22ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose input and output labels with individual variable names\n",
    "input_vars_label = '-'.join([v.replace('_n','').upper() for v in vars_for_model_input])\n",
    "output_vars_label = '-'.join([v.replace('_n','').upper() for v in vars_for_model_output])\n",
    "#vars_for_model_input+vars_for_model_output\n",
    "\n",
    "# compose cas name with input and output labels. Its a title or label indentifying the case to be used in figures, etc.\n",
    "case_label = f\"{output_vars_label} from {input_vars_label} [Net1-RndData-wTest]\"\n",
    "# name indentifying converted to label for filename (bo white spacen no '+', no '[' nieder ']' and in lower case. To be used en output and figure filenames\n",
    "case_label_prnt = case_label.replace(' ','-').replace('+','-').replace('[','').replace(']','').lower()\n",
    "print(f\"Case label ....... {case_label},\\nCase_label_prnt .. {case_label_prnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef786568-be96-4bfd-84b8-1d833ee1ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim  = X_train.shape[1]\n",
    "output_dim = Y_train.shape[1]\n",
    "hidden_dim = [10,8,5]\n",
    "\n",
    "optimizer = 'Adam'\n",
    "learning_rate = 0.001\n",
    "model_filename = 'my_model-T{test_ident}-4hc'\n",
    "\n",
    "# new NN model\n",
    "model = build_n_model(input_dim, output_dim, hidden_dim)\n",
    "\n",
    "# Compiling model\n",
    "print('-- compiling model with loss and optimizer functions')\n",
    "if optimizer.lower() == 'adam' :\n",
    "    if learning_rate is None :\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    else:\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizers.Adam(learning_rate=learning_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e31d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_figs = True\n",
    "save_nets = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4913aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the network (giving more arguments to the function)\n",
    "rankdir_value = 'LR' # 'LR' or 'TB'  (left-right or top-bottom)\n",
    "plot_model_args = { 'show_shapes':False, 'show_layer_names':True, 'rankdir':rankdir_value }\n",
    "if save_figs :\n",
    "    figs_filename = f\"Net_Model5variaveis\"\n",
    "    figs_filename += f\"{model_filename}-{input_dim}-{'-'.join(map(str, hidden_dim))}-{output_dim}\"\n",
    "    figs_filename += f\"_case-{case_label_prnt}_model-{rankdir_value}.png\"\n",
    "    figs_filepath = figs_filename\n",
    "    print(\"-- saving figure in file ... '{}'\".format(figs_filepath))\n",
    "    plot_model_args['to_file'] = figs_filepath    # ajoute le nom du fichier aux arguments \n",
    "\n",
    "plot_model(model, **plot_model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f668d5-77a1-4758-8fa6-be91531eb727",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8ee0f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# the %%time  at first line of a Notebook's cell tells the kernel\n",
    "# to evaluate execution time of the cell's code\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 1024*4\n",
    "\n",
    "# Training\n",
    "print(f'Begining training ...')\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=batch_size, epochs=epochs, verbose='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_dic['loss'] = history.history['loss']\n",
    "case_dic['val_loss'] = history.history['val_loss']\n",
    "\n",
    "# save trained model\n",
    "if save_nets :\n",
    "    #model_filepath = os.path.join(nets_dir,model_filename)\n",
    "    model_filepath = f\"{model_filename}_net\"\n",
    "    print(f\"-- saving trained model in file '{model_filepath}'...\")\n",
    "    model.save(model_filepath)\n",
    "\n",
    "    #case_info_file = os.path.join(nets_dir,f\"{model_filename}_info.p\")\n",
    "    case_info_file = f\"{model_filename}_info.p\"\n",
    "    print(f\"-- saving case inrormation model in file '{case_info_file}'...\")\n",
    "    pickle.dump( case_dic, open( case_info_file, \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddae4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.suptitle(f\"{case_label}: Loss curves\",y=0.98,size=\"large\");\n",
    "\n",
    "if save_figs :\n",
    "    figs_filename = f\"Fig_loss-curves\"\n",
    "    figs_filename += f\"-{model_filename}-{input_dim}-{'-'.join(map(str, hidden_dim))}-{output_dim}\"\n",
    "    figs_filename += f\"_case-{case_label_prnt}\"\n",
    "    #figs_filepath = os.path.join(figs_dir,figs_filename)\n",
    "    figs_filepath = f\"{figs_filename}.png\"\n",
    "    print(\"-- saving figure in file ... '{}'\".format(figs_filepath))\n",
    "    plt.savefig(figs_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on Validation data\n",
    "val_y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3147a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# denorm y\n",
    "y_stat_mean, y_stat_std,  = case_dic['pco2_stat']\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(nrows=1,ncols=2,sharex=True,sharey=True,figsize=(16,7))\n",
    "\n",
    "# Prediction on Validation data\n",
    "val_y_pred = model.predict(X_val)\n",
    "\n",
    "y_obs_dnorm = Y_val.flatten()*y_stat_std + y_stat_mean\n",
    "y_pred_dnorm = val_y_pred.flatten()*y_stat_std + y_stat_mean\n",
    "\n",
    "rmse = np.sqrt(((y_obs_dnorm - y_pred_dnorm)**2).mean())\n",
    "corr, _ = pearsonr(y_obs_dnorm, y_pred_dnorm)\n",
    "\n",
    "ax1.scatter(y_obs_dnorm,y_pred_dnorm,s=2, alpha=0.1)\n",
    "ax1.set_xlabel('Observado')\n",
    "ax1.set_ylabel('Calculado')\n",
    "\n",
    "# linear regression (red line) \n",
    "m, b = np.polyfit(y_obs_dnorm,y_pred_dnorm, 1)\n",
    "x = np.array([100, 500])\n",
    "ax1.plot(x, m*x + b, c='r')\n",
    "\n",
    "ax1.axis('image')\n",
    "\n",
    "lax = ax1.axis()\n",
    "# plot diagonal\n",
    "ax1.plot([min((lax[0],lax[2])),min((lax[1],lax[3]))],[min((lax[0],lax[2])),min((lax[1],lax[3]))],ls='-',lw=0.5,c='k')\n",
    "ax1.axis(lax)\n",
    "\n",
    "ax1.set_title(f\"VAL Obs Vs. Pred (RMS: {rmse:.3f}, Pearson's corr: {corr:.2f} \");\n",
    "\n",
    "# Prediction on TEST data\n",
    "test_y_pred = model.predict(X_test)\n",
    "\n",
    "y_obs_dnorm = Y_test.flatten()*y_stat_std + y_stat_mean\n",
    "y_pred_dnorm = test_y_pred.flatten()*y_stat_std + y_stat_mean\n",
    "\n",
    "rmse = np.sqrt(((y_obs_dnorm - y_pred_dnorm)**2).mean())\n",
    "corr, _ = pearsonr(y_obs_dnorm, y_pred_dnorm)\n",
    "\n",
    "ax2.scatter(y_obs_dnorm,y_pred_dnorm,s=2, alpha=0.1)\n",
    "ax2.set_xlabel('Obs')\n",
    "ax2.set_ylabel('Pred')\n",
    "\n",
    "# linear regression (red line) \n",
    "m, b = np.polyfit(y_obs_dnorm,y_pred_dnorm, 1)\n",
    "x = np.array([100, 500])\n",
    "ax2.plot(x, m*x + b, c='r')\n",
    "\n",
    "ax2.axis('image')\n",
    "\n",
    "lax = ax2.axis()\n",
    "# plot diagonal\n",
    "ax2.plot([min((lax[0],lax[2])),min((lax[1],lax[3]))],[min((lax[0],lax[2])),min((lax[1],lax[3]))],ls='-',lw=0.5,c='k')\n",
    "ax2.axis(lax)\n",
    "\n",
    "ax2.set_title(f\"TEST Obs Vs. Pred (RMS: {rmse:.3f}, Pearson's corr: {corr:.2f} \");\n",
    "\n",
    "plt.suptitle(f\"{case_label} Obs Vs. Pred \",size='xx-large');\n",
    "plt.savefig(f'validação_e_teste-{model_filename}.png',dpi=300,tight_layout= True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
